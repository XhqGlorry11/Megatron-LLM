#ï¼ /usr/bin/bash

CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 /usr/bin/python -m torch.distributed.launch \
--nproc_per_node=8 \
--master_port=58417 \
--use_env \
/home/xinghq/megatron-llm/finetune.py \
--tensor_model_parallel_size=4 \
--pipeline_model_parallel_size=2 \
--save=/hstore/llm_train_val/megatron-llm/fintune_llama/checkpoints \
--load=/hstore/llm_train_val/megatron-llm/fintune_llama/checkpoints \
--train_data_path=/hstore/llm_data/The_Pile/train/tokenization/train_data_text_document \
--test_data_path=/hstore/llm_data/The_Pile/test/tokenization/test_data_text_document \
--valid_data_path=/hstore/llm_data/The_Pile/validate/tokenization/validate_data_text_document \
--model_name=llama2 \
--tokenizer_type=HFTokenizer \
--vocab_file=/home/xinghq/megatron-llm/tokenizer/neox_20B_tokenizer.json \
--fp16 \
--use_flash_attn \
--micro_batch_size=3 \
--global_batch_size=192 \
--sequence_parallel \
--recompute_granularity=selective \
--use_checkpoint_args \
--num_workers=32 \
--train_iters=100000 \
--lr_decay_style=cosine \
--lr_warmup_fraction=0.01 \
--lr=0.0003 \
--min_lr=0.00003 \
--no_new_tokens \
--num_layers=24 \
--hidden_size=2048 \
--num_attention_heads=16 \
--ffn_hidden_size=5632 \
--make_vocab_size_divisible_by=128 \
--seq_length=4096 \
--max_position_embeddings=4096 \
--position_embedding_type=rotary \
--glu_activation=swiglu \
--use_rms_norm \
--layernorm_epsilon=1e-5 \
--no_tie_embed_logits \
--no_bias_gelu_fusion \
--no_bias_dropout_fusion \
--hidden_dropout=0.0 \
--attention_dropout=0.0 \
--log_interval=1 \
--save_interval=10000 \
--eval_interval=1000 \
--log_params_norm \
--log_num_zeros_in_grad \
--wandb_logger \
--wandb_project=llama-megatron \
--wandb_id=3 \
--wandb_api_key=fac46169cec8e164a47ed1c71199e3e8e9f02cc5 \
2>&1 |tee debug.txt
